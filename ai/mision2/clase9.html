<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JavaScript</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../../css/estilos.css">
</head>
    <body>
        <div class="container">
            <div class="slide">
                <h1>Introducción al Procesamiento del Lenguaje Natural</h1>
                <p>Este notebook explora diversos temas de PNL utilizando la biblioteca NLTK en Python.</p>
              </div>
              
              <div class="slide">
                <h2>Tokenización de Palabras</h2>
                <p>La tokenización de palabras es el proceso de dividir un texto en palabras individuales.</p>
  <pre><code>
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')
texto = "Hola, ¿cómo estás? Espero que estés bien."
tokens = word_tokenize(texto)
print(tokens)
  </code></pre>
              </div>
              
              <div class="slide">
                <h2>Tokenización de Frases</h2>
                <p>La tokenización de frases es el proceso de dividir un texto en frases individuales.</p>
  <pre><code>
from nltk.tokenize import sent_tokenize

texto = "Hola, ¿cómo estás? Espero que estés bien. Nos vemos luego."
frases = sent_tokenize(texto)
print(frases)
  </code></pre>
              </div>
              
              <div class="slide">
                <h2>Eliminación de Stop Words</h2>
                <p>Las stop words son palabras comunes que se eliminan del texto para mejorar el análisis.</p>
  <pre><code>
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')

texto = "El procesamiento de lenguaje natural es todo un mundo y útil en muchas aplicaciones."
stop_words = set(stopwords.words('spanish'))
palabras = word_tokenize(texto)

palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stop_words]
print(palabras_filtradas)
  </code></pre>
              </div>
              
              <div class="slide">
                <h2>TF-IDF</h2>
                <p>TF-IDF (Term Frequency-Inverse Document Frequency) es una técnica para calcular la importancia de una palabra en un documento en relación con una colección de documentos.</p>
  <pre><code>
from sklearn.feature_extraction.text import TfidfVectorizer

documentos = [
    "El procesamiento de lenguaje natural es todo un mundo.",
    "El lenguaje natural permite la comunicación entre humanos y máquinas.",
    "Las técnicas de NLP mejoran la comprensión del texto."
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documentos)

print(tfidf_matrix.toarray())
print(vectorizer.get_feature_names_out())
  </code></pre>
              </div>
              
              <div class="slide">
                <h2>Word2Vec</h2>
                <p>Word2Vec es una técnica para aprender representaciones vectoriales de palabras (word embeddings).</p>
                <pre><code>
from gensim.models import Word2Vec

sentences = [["el", "procesamiento", "de", "lenguaje", "natural", "es", "todo", "un", "mundo"],
              ["el", "lenguaje", "natural", "permite", "la", "comunicación", "entre", "humanos", "y", "máquinas"]]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
vector = model.wv['lenguaje']
print(vector)
                </code></pre>
              </div>
              
              <div class="slide">
                <h2>N-grams</h2>
                <p>Los N-grams son secuencias de N palabras consecutivas en un texto.</p>
                <pre><code>
import nltk
from nltk.util import ngrams
from collections import Counter

nltk.download('punkt')

texto = "El procesamiento de lenguaje natural es fascinante y útil en muchas aplicaciones."
tokens = nltk.word_tokenize(texto)

bigrams = list(ngrams(tokens, 2))
bigram_freq = Counter(bigrams)

print(bigrams)
print(bigram_freq)
                </code></pre>
              </div>
              
              <div class="slide">
                <h2>Reconocimiento de Entidades Nombradas (NER)</h2>
                <p>NER es la tarea de identificar y clasificar entidades nombradas en un texto, como personas, organizaciones, ubicaciones, etc.</p>
                <pre><code>
import spacy

nlp = spacy.load("es_core_news_sm")

texto = "Apple lanzó el nuevo iPhone en California el 13 de octubre"
doc = nlp(texto)

for entidad in doc.ents:
    print(entidad.text, entidad.label_)
                </code></pre>
              </div>
              
              <div class="slide">
                <h2>Análisis de Sentimientos</h2>
                <p>El análisis de sentimientos es la tarea de determinar la polaridad emocional de un texto (positivo, negativo o neutral).</p>
                <pre><code>
from transformers import pipeline

clasificador = pipeline("sentiment-analysis")

texto = "te odio mucho"
resultado = clasificador(texto)
print(resultado)
                </code></pre>
              </div>
              
              <div class="slide">
                <h2>Stemming</h2>
                <p>Stemming es el proceso de reducir una palabra a su raíz o stem.</p>
                <pre><code>
import nltk
from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer("spanish")
palabras = ["corriendo", "jugando", "vida", "correis", "ostias", "jodido","mandado"]
stems = [stemmer.stem(palabra) for palabra in palabras]
print(stems)
                </code></pre>
              </div>
              
              <div class="slide">
                <h2>Lematización</h2>
                <p>La lematización es el proceso de reducir una palabra a su forma base o lema.</p>
                <pre><code>
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
palabras = ["running", "played", "better", "dogs"]
lemmas = [lemmatizer.lemmatize(palabra, pos=wordnet.VERB) for palabra in palabras]
print(lemmas)
                </code></pre>
              </div>
              
              <div class="slide">
                <h2>Análisis Sintáctico</h2>
                <p>El análisis sintáctico es el proceso de analizar la estructura gramatical de una oración.</p>
                <pre><code>
import spacy

nlp = spacy.load("es_core_news_sm")
texto = "El gato negro corre rápidamente por el jardín."
doc = nlp(texto)

for token in doc:
    print(f"Palabra: {token.text}, Etiqueta gramatical: {token.pos_}, Dependencia: {token.dep_}")
                </code></pre>
              </div>
              
              <div class="slide">
                <h2>Similitud Semántica</h2>
                <p>La similitud semántica es una medida de la relación entre dos palabras o frases en función de su significado.</p>
                <pre><code>
import spacy

nlp = spacy.load("es_core_news_md")
palabra1 = nlp("suba pa arriba las escaleras")
palabra2 = nlp("spacy es un modelo de lenguaje natural")
print(f"Similitud entre {palabra1} y {palabra2}': {palabra1.similarity(palabra2)}")
                </code></pre>
              </div>

        <div class="nav">
            <button onclick="prevSlide()">Anterior</button>
            <button onclick="nextSlide()">Siguiente</button>
        </div>
    </div>
    <script src="../../js/slide.js"></script>
    </body>
</html>
